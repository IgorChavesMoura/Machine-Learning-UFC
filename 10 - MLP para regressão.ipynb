{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP para regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'data/regression_example.txt' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-14ffc86423c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \"\"\"\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mdata_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/regression_example.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'data/regression_example.txt' does not exist"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import neural_networks as neural_network\n",
    "import common as com\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "np.random.seed(12345)\n",
    "\n",
    "train_ratio = 0.8\n",
    "validation_ratio = 0.2\n",
    "\n",
    "\"\"\"\n",
    "np.random.seed(12345)\n",
    "x = 2 * np.random.rand(500*2).reshape((500,2)) - 1\n",
    "y = (np.sin(x[:,0]) ** 2) * (np.cos(x[:,1]) ** 2) + x[:,0] * x[:,1] ** 3\n",
    "y += np.random.normal(scale=0.01 * np.std(y), size=y.shape)   \n",
    "import pandas as pd\n",
    "df = pd.DataFrame(np.hstack((x, y[:,None])))\n",
    "df.to_csv('data/regression_example.txt', index=False, header=None)\n",
    "\"\"\"\n",
    "\n",
    "data_table = pd.read_csv('data/regression_example.txt', header=None)\n",
    "x = data_table.drop(data_table.columns[-1], axis=1).values\n",
    "y = data_table.iloc[:,data_table.columns[-1]].values  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divisão treino/teste e normalização dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_data = True\n",
    "\n",
    "np.random.seed(12345)\n",
    "\n",
    "number_train = int(np.ceil(train_ratio*(x.shape[0])))\n",
    "random_index = np.random.permutation(x.shape[0])\n",
    "train_index = random_index[:number_train+1]\n",
    "test_index = random_index[number_train+1:]\n",
    "\n",
    "x_train = x[train_index]\n",
    "x_test = x[test_index]\n",
    "y_train = y[train_index]\n",
    "y_test = y[test_index]\n",
    "\n",
    "print(\"Número de padrões de treinamento: %d\" % len(train_index))\n",
    "print(\"Número de padrões de teste: %d\" % len(test_index))\n",
    "\n",
    "if normalize_data:\n",
    "\n",
    "    x_train_mean = np.mean(x_train, axis=0)\n",
    "    x_train = x_train - x_train_mean\n",
    "    x_train_std = np.std(x_train, axis=0)\n",
    "    x_train /= x_train_std\n",
    "    y_train_mean = np.mean(y_train, axis=0)\n",
    "    y_train = y_train - y_train_mean\n",
    "    y_train_std = np.std(y_train, axis=0)\n",
    "    y_train /= y_train_std\n",
    "\n",
    "    x_test = (x_test - x_train_mean) / x_train_std\n",
    "    y_test = (y_test - y_train_mean) / y_train_std\n",
    "\n",
    "    x = (x - x_train_mean) / x_train_std\n",
    "    y = (y - y_train_mean) / y_train_std\n",
    "    \n",
    "    if x_train.shape[1] == 1:\n",
    "        fig = plt.figure(figsize=com.figsize)\n",
    "        plt.plot(x_train, y_train, 'ob', markersize=com.markersize)\n",
    "        plt.plot(x_test, y_test, 'xb', markersize=com.markersize)\n",
    "        plt.show()\n",
    "        xlim = fig.gca().get_xlim() \n",
    "        ylim = fig.gca().get_ylim() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação do conjunto de validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_validation = int(np.ceil(validation_ratio*(x_train.shape[0])))\n",
    "number_train_validation = x_train.shape[0] - number_validation\n",
    "\n",
    "random_index = np.random.permutation(x_train.shape[0])\n",
    "train_index = random_index[:number_train_validation+1]\n",
    "validation_index = random_index[number_train_validation+1:number_train_validation+number_validation+1]\n",
    "\n",
    "x_train_validation, x_validation = x_train[train_index], x_train[validation_index]\n",
    "y_train_validation, y_validation = y_train[train_index], y_train[validation_index]\n",
    "\n",
    "print(\"Conjunto de treinamento: %d amostras\" % y_train_validation.shape[0])\n",
    "print(\"Conjunto de validação: %d amostras\" % y_validation.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solução via MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_nodes = 100\n",
    "activation = 'relu'\n",
    "output = 'regression'\n",
    "alpha = 10**-2\n",
    "mini_batch_size = 32\n",
    "num_epochs = 10 * mini_batch_size\n",
    "momentum = 0.9\n",
    "weight_decay = 0\n",
    "model_mlp = neural_network.mlp_train(x=x_train_validation, y=y_train_validation,\n",
    "                                     x_validation=x_validation, y_validation=y_validation,\n",
    "                                     num_hidden_nodes=num_hidden_nodes, activation=activation, output=output,\n",
    "                                     num_epochs=num_epochs, alpha=alpha, mini_batch_size=mini_batch_size,\n",
    "                                     momentum=momentum, weight_decay=weight_decay, compute_loss=True)  \n",
    "\n",
    "loss_history_mlp = model_mlp['loss_history']\n",
    "loss_mlp = loss_history_mlp[-1]\n",
    "validation_loss_history_mlp = model_mlp['validation_loss_history']\n",
    "valdiation_loss_mlp = validation_loss_history_mlp[-1]\n",
    "\n",
    "if len(loss_history_mlp) > 1:\n",
    "    plt.figure(figsize=com.figsize)\n",
    "    plt.rcParams.update({'font.size': com.fontsize})\n",
    "    plt.plot(range(1,len(loss_history_mlp)+1), loss_history_mlp, '-k', label='Training loss')\n",
    "    plt.plot(range(1,len(validation_loss_history_mlp)+1), validation_loss_history_mlp, '-r', label='Validation loss')\n",
    "    plt.xlabel('Epochs', fontsize=com.fontsize)\n",
    "    plt.ylabel('Loss', fontsize=com.fontsize)\n",
    "    plt.legend()\n",
    "\n",
    "model_mlp = neural_network.mlp_train(x=x_train, y=y_train,\n",
    "                                     num_hidden_nodes=num_hidden_nodes, activation=activation, output=output,\n",
    "                                     num_epochs=num_epochs, alpha=alpha, mini_batch_size=mini_batch_size,\n",
    "                                     momentum=momentum, weight_decay=weight_decay, compute_loss=False)    \n",
    " \n",
    "pred_train = neural_network.mlp_predict(model_mlp, x_train)\n",
    "mse_train = np.mean((y_train - pred_train)**2)  \n",
    "pred_test = neural_network.mlp_predict(model_mlp, x_test)\n",
    "mse_test = np.mean((y_test - pred_test)**2)   \n",
    "\n",
    "print(\"MSE no treinamento: %.2e\" % mse_train)\n",
    "print(\"MSE no teste: %.2e\" % mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_example(x0, x1):\n",
    "    if normalize_data:\n",
    "        x0 = x0 * x_train_std[0] + x_train_mean[0]\n",
    "        x1 = x1 * x_train_std[1] + x_train_mean[1]\n",
    "    y = (np.sin(x0) ** 2) * (np.cos(x1) ** 2) + x0 * x1 ** 3\n",
    "    if normalize_data:\n",
    "        y = (y - y_train_mean) / y_train_std\n",
    "    return y\n",
    "\n",
    "def regression_example_grid(x):\n",
    "    x0 = x[0]\n",
    "    x1 = x[1]            \n",
    "    z = x0.copy()    \n",
    "    for i in range(x0.shape[0]):\n",
    "        for j in range(x0.shape[1]):\n",
    "            z[i,j] = regression_example(x0[i,j], x1[i,j])\n",
    "    return z\n",
    "\n",
    "def mlp_grid(x):\n",
    "    x0 = x[0]\n",
    "    x1 = x[1]            \n",
    "    z = x0.copy()    \n",
    "    for i in range(x0.shape[0]):\n",
    "        for j in range(x0.shape[1]):\n",
    "            z[i,j] = neural_network.mlp_predict(model_mlp, np.array([x0[i,j], x1[i,j]]).reshape((1,2)))    \n",
    "    return z\n",
    "\n",
    "x_grid = np.meshgrid(np.linspace(np.min(x[:,0]), np.max(x[:,0]), 100),\n",
    "                     np.linspace(np.min(x[:,1]), np.max(x[:,1]), 100))\n",
    "y_grid = regression_example_grid(x_grid)\n",
    "plt.figure(figsize=com.figsize)\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(x_grid[0], x_grid[1], y_grid, cmap='binary')\n",
    "plt.title('Original function')\n",
    "#plt.savefig('figs/linear_reg_surf.png', dpi=com.dpi)\n",
    "plt.show()\n",
    "\n",
    "pred_grid = mlp_grid(x_grid)\n",
    "plt.figure(figsize=com.figsize)\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(x_grid[0], x_grid[1], pred_grid, cmap='binary')\n",
    "plt.title('MLP learned function')\n",
    "#plt.savefig('figs/linear_reg_surf.png', dpi=com.dpi)\n",
    "plt.show()      \n",
    "\n",
    "plt.figure(figsize=com.figsize)\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(x_grid[0], x_grid[1], y_grid - pred_grid, cmap='binary')\n",
    "plt.title('MLP errors')\n",
    "#plt.savefig('figs/linear_reg_surf.png', dpi=com.dpi)\n",
    "plt.show()   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
